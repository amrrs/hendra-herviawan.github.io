<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Titanic Survival Analysis | M Hendra Herviawan
</title>
  <link rel="canonical" href="/titanic_survival_analysis.html">


  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">

  
  <meta name="description" content="Titanic Survival Analysis">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="/">M Hendra Herviawan</a></h1>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="https://www.kaggle.com/hendraherviawan/kernels" target="_blank">Kaggle</a></li>
          <li class="list-inline-item"><a href="https://www.linkedin.com/in/m-hendra-herviawan-0a49733/" target="_blank">Linkedin</a></li>
          <li class="list-inline-item"><a href="https://github.com/hendra-herviawan" target="_blank">Github</a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  Titanic Survival Analysis
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2017-07-22T00:00:00+07:00">
          <i class="fa fa-clock-o"></i>
          Sat 22 July 2017
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="/category/data-science.html">Data Science</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="/author/m-hendra-herviawan.html">M Hendra Herviawan</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="/tag/r.html">#R</a>          </li>
      </ul>
    </header>
    <div class="content">
      <div class="highlight"><pre><span></span><span class="kp">set.seed</span><span class="p">(</span><span class="m">1111</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;dplyr&#39;</span><span class="p">)</span> 
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;readr&#39;</span><span class="p">)</span> 
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;pander&#39;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;ggplot2&#39;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;ggthemes&#39;</span><span class="p">)</span> 
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;VIM&#39;</span><span class="p">)</span> 
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;caret&#39;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;rpart&#39;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;rpart.plot&#39;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;glmnet&#39;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span><span class="s">&#39;xgboost&#39;</span><span class="p">)</span> 
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">#Dataset download from https://www.kaggle.com/c/titanic/data</span>
train <span class="o">&lt;-</span> read_csv<span class="p">(</span><span class="s">&#39;_dataset//train.csv&#39;</span><span class="p">)</span>
test  <span class="o">&lt;-</span> read_csv<span class="p">(</span><span class="s">&#39;_dataset//test.csv&#39;</span><span class="p">)</span>

train<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>## 
## -------------------------------------------------------------------------------
##  PassengerId   Survived   Pclass               Name                Sex     Age 
## ------------- ---------- -------- ------------------------------ -------- -----
##       1           0         3        Braund, Mr. Owen Harris       male    22  
## 
##       2           1         1       Cumings, Mrs. John Bradley    female   38  
##                                      (Florence Briggs Thayer)                  
## 
##       3           1         3         Heikkinen, Miss. Laina      female   26  
## 
##       4           1         1      Futrelle, Mrs. Jacques Heath   female   35  
##                                          (Lily May Peel)                       
## 
##       5           0         3        Allen, Mr. William Henry      male    35  
## -------------------------------------------------------------------------------
## 
## Table: Table continues below
## 
##  
## -------------------------------------------------------------
##  SibSp   Parch        Ticket        Fare    Cabin   Embarked 
## ------- ------- ------------------ ------- ------- ----------
##    1       0        A/5 21171       7.25     NA        S     
## 
##    1       0         PC 17599       71.28    C85       C     
## 
##    0       0     STON/O2. 3101282   7.925    NA        S     
## 
##    1       0          113803        53.1    C123       S     
## 
##    0       0          373450        8.05     NA        S     
## -------------------------------------------------------------
</pre></div>


<h3>1. Can you explain what are the characteristics of this analysis?</h3>
<p>It is a type of <strong>supervised learning</strong>, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into a categories. When there are only two categories the problem is known as binary classification. Binary classification involves classifying the data into two groups (Survived vs Non Survived),  e.g. whether or not a customer buys a particular product or not (Yes/No), based on independent variables such as gender, age, location etc.</p>
<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-3-1.png"><!-- --></p>
<h3>2. What are the informations in data that can be considered as features?</h3>
<p>In general we can categorize feature as structured vs unstructure. structured data is comprised of clearly defined data types (numeric, string, category, etc) whose pattern makes them easily understood. while unstructured data – “everything else” – is comprised of data that is usually not as easily understood and need preprocessing to gain understanding.</p>
<div class="highlight"><pre><span></span><span class="kp">summary</span><span class="p">(</span>combine<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>##   PassengerId   Survived   Pclass      Name               Sex     
##  Min.   :   1   0   :549   1:323   Length:1309        female:466  
##  1st Qu.: 328   1   :342   2:277   Class :character   male  :843  
##  Median : 655   NA&#39;s:418   3:709   Mode  :character               
##  Mean   : 655                                                     
##  3rd Qu.: 982                                                     
##  Max.   :1309                                                     
##                                                                   
##       Age            SibSp            Parch          Ticket         
##  Min.   : 0.17   Min.   :0.0000   Min.   :0.000   Length:1309       
##  1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   Class :character  
##  Median :28.00   Median :0.0000   Median :0.000   Mode  :character  
##  Mean   :29.88   Mean   :0.4989   Mean   :0.385                     
##  3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000                     
##  Max.   :80.00   Max.   :8.0000   Max.   :9.000                     
##  NA&#39;s   :263                                                        
##       Fare            Cabin           Embarked  
##  Min.   :  0.000   Length:1309        C   :270  
##  1st Qu.:  7.896   Class :character   Q   :123  
##  Median : 14.454   Mode  :character   S   :914  
##  Mean   : 33.295                      NA&#39;s:  2  
##  3rd Qu.: 31.275                                
##  Max.   :512.329                                
##  NA&#39;s   :1
</pre></div>


<p>In the Titanic dataset PassengerId, Name, Ticket can be considered as unstructure becouse we need to preprocessing to gain understanding what is the meaning behind sequence of digit/char.</p>
<p>To help examine importance feature in structured data we can use decision tree to build feature importance. importance provides a score that indicates how useful or valuable each feature was in the construction of the  decision trees.</p>
<div class="highlight"><pre><span></span><span class="c1">#Use Decision Tree to get importance feature</span>
x <span class="o">&lt;-</span> select<span class="p">(</span>train<span class="p">,</span> <span class="o">-</span><span class="kt">c</span><span class="p">(</span>PassengerId<span class="p">,</span> Name<span class="p">,</span> Ticket<span class="p">,</span> Cabin<span class="p">))</span>
Model_DT<span class="o">=</span>rpart<span class="p">(</span>Survived<span class="o">~</span><span class="m">.</span><span class="p">,</span>data<span class="o">=</span>x<span class="p">,</span>method<span class="o">=</span><span class="s">&quot;class&quot;</span><span class="p">)</span>
rpart.plot<span class="p">(</span>Model_DT<span class="p">,</span>extra <span class="o">=</span>  <span class="m">3</span><span class="p">,</span>fallen.leaves <span class="o">=</span> <span class="bp">T</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/naive-decisionTree-1.png"><!-- --></p>
<h3>3. Do you need to create any new features?</h3>
<p>When looking at the decision tree above it appears that <strong>sex, age &amp; pclass</strong> are the 3 most important features. When creating the Decison tree we only can use structured data becouse limitation of decision tree algoritma. </p>
<div class="highlight"><pre><span></span><span class="kp">head</span><span class="p">(</span>combine<span class="o">$</span>Name<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>## [1] &quot;Braund, Mr. Owen Harris&quot;                            
## [2] &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;
## [3] &quot;Heikkinen, Miss. Laina&quot;                             
## [4] &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot;       
## [5] &quot;Allen, Mr. William Henry&quot;                           
## [6] &quot;Moran, Mr. James&quot;
</pre></div>


<p>If we examine closely <em>Name</em> Feature it has pattern "last name"+"Title"+"First name". We can use title as category feature because title represents <strong>sex, age &amp; pclass/wealth</strong> the 3 most importance feature based on decision tree.</p>
<div class="highlight"><pre><span></span>names <span class="o">&lt;-</span> combine<span class="o">$</span>Name
Title <span class="o">&lt;-</span>  <span class="kp">gsub</span><span class="p">(</span><span class="s">&quot;^.*, (.*?)\\..*$&quot;</span><span class="p">,</span> <span class="s">&quot;\\1&quot;</span><span class="p">,</span> <span class="kp">names</span><span class="p">)</span>
combine<span class="o">$</span>Title <span class="o">&lt;-</span> Title

combine<span class="o">$</span>Title<span class="p">[</span>combine<span class="o">$</span>Title <span class="o">==</span> <span class="s">&#39;Mlle&#39;</span><span class="p">]</span>        <span class="o">&lt;-</span> <span class="s">&#39;Miss&#39;</span> 
combine<span class="o">$</span>Title<span class="p">[</span>combine<span class="o">$</span>Title <span class="o">==</span> <span class="s">&#39;Ms&#39;</span><span class="p">]</span>          <span class="o">&lt;-</span> <span class="s">&#39;Miss&#39;</span>
combine<span class="o">$</span>Title<span class="p">[</span>combine<span class="o">$</span>Title <span class="o">==</span> <span class="s">&#39;Mme&#39;</span><span class="p">]</span>         <span class="o">&lt;-</span> <span class="s">&#39;Mrs&#39;</span> 
combine<span class="o">$</span>Title<span class="p">[</span>combine<span class="o">$</span>Title <span class="o">==</span> <span class="s">&#39;Lady&#39;</span><span class="p">]</span>          <span class="o">&lt;-</span> <span class="s">&#39;Miss&#39;</span>
combine<span class="o">$</span>Title<span class="p">[</span>combine<span class="o">$</span>Title <span class="o">==</span> <span class="s">&#39;Dona&#39;</span><span class="p">]</span>          <span class="o">&lt;-</span> <span class="s">&#39;Miss&#39;</span>

<span class="c1"># Titles with very low cell counts to be combined to &quot;others&quot; level</span>
others_Title <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&#39;the Countess&#39;</span><span class="p">,</span><span class="s">&#39;Capt&#39;</span><span class="p">,</span> <span class="s">&#39;Col&#39;</span><span class="p">,</span> <span class="s">&#39;Don&#39;</span><span class="p">,</span> 
                <span class="s">&#39;Dr&#39;</span><span class="p">,</span> <span class="s">&#39;Major&#39;</span><span class="p">,</span> <span class="s">&#39;Rev&#39;</span><span class="p">,</span> <span class="s">&#39;Sir&#39;</span><span class="p">,</span> <span class="s">&#39;Jonkheer&#39;</span><span class="p">)</span>

combine<span class="o">$</span>Title<span class="p">[</span>combine<span class="o">$</span>Title <span class="o">%in%</span> others_Title<span class="p">]</span>  <span class="o">&lt;-</span> <span class="s">&#39;Others&#39;</span>
combine<span class="o">$</span>Title <span class="o">&lt;-</span> <span class="kp">factor</span><span class="p">(</span>combine<span class="o">$</span>Title<span class="p">)</span>
<span class="c1">#table(combine$Title)</span>
</pre></div>


<p>Lets check who among Mr, Master, Miss having a better survival rate</p>
<div class="highlight"><pre><span></span> ggplot<span class="p">(</span>combine<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">891</span><span class="p">,],</span>aes<span class="p">(</span>x <span class="o">=</span> Title<span class="p">,</span>fill<span class="o">=</span><span class="kp">factor</span><span class="p">(</span>Survived<span class="p">)))</span> <span class="o">+</span>
  geom_bar<span class="p">()</span> <span class="o">+</span>
  ggtitle<span class="p">(</span><span class="s">&quot;Title vs Survival rate&quot;</span><span class="p">)</span><span class="o">+</span>
  xlab<span class="p">(</span><span class="s">&quot;Title&quot;</span><span class="p">)</span> <span class="o">+</span>
  ylab<span class="p">(</span><span class="s">&quot;Total Count&quot;</span><span class="p">)</span> <span class="o">+</span>
  labs<span class="p">(</span>fill <span class="o">=</span> <span class="s">&quot;Survived&quot;</span><span class="p">)</span> 
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-7-1.png"><!-- --></p>
<h3>4. Do you need to scale any of the features?</h3>
<p>We need to do scale or normalization when there is class imbalance or use algoritma that sensitive to data skewness like Neural Network/Deeplearning. In this assisgment we dont need to to data scaling becouse we will use algortima that based on decision tree.  </p>
<h3>5. Please recap the missing values on the dataset, What will you do with the missing data?</h3>
<div class="highlight"><pre><span></span>aggr<span class="p">(</span>combine<span class="p">,</span> prop <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span> combined <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span> numbers <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span> sortVars <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span> sortCombs <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/fig.missing-1.png"><!-- --></p>
<div class="highlight"><pre><span></span>## 
##  Variables sorted by number of missings: 
##     Variable Count
##        Cabin  1014
##     Survived   418
##          Age   263
##     Embarked     2
##         Fare     1
##  PassengerId     0
##       Pclass     0
##         Name     0
##          Sex     0
##        SibSp     0
##        Parch     0
##       Ticket     0
##        Title     0
</pre></div>


<h4>5.1 Cabin</h4>
<p>We will delete this feature becouse 1014 is missing from 1309. </p>
<h4>5.2 Embarked</h4>
<div class="highlight"><pre><span></span>filter<span class="p">(</span>combine<span class="p">,</span> <span class="kp">is.na</span><span class="p">(</span>Embarked<span class="p">))</span> 
</pre></div>


<div class="highlight"><pre><span></span>## 
## ----------------------------------------------------------------------------
##  PassengerId   Survived   Pclass             Name               Sex     Age 
## ------------- ---------- -------- --------------------------- -------- -----
##      62           1         1         Icard, Miss. Amelie      female   38  
## 
##      830          1         1      Stone, Mrs. George Nelson   female   62  
##                                         (Martha Evelyn)                     
## ----------------------------------------------------------------------------
## 
## Table: Table continues below
## 
##  
## ----------------------------------------------------------
##  SibSp   Parch   Ticket   Fare   Cabin   Embarked   Title 
## ------- ------- -------- ------ ------- ---------- -------
##    0       0     113572    80     B28       NA      Miss  
## 
##    0       0     113572    80     B28       NA       Mrs  
## ----------------------------------------------------------
</pre></div>


<p>PassengerId 62 &amp; 830 have same Ticket, Fare and Cabin number, Look like they came from same Embarked. We can use median Fare for every Embarked to fill this missing value. </p>
<div class="highlight"><pre><span></span><span class="c1"># Get rid of our missing passenger IDs</span>
embark_fare <span class="o">&lt;-</span> combine <span class="o">%&gt;%</span>
  filter<span class="p">(</span>PassengerId <span class="o">!=</span> <span class="m">62</span> <span class="o">&amp;</span> PassengerId <span class="o">!=</span> <span class="m">830</span><span class="p">)</span>

<span class="c1"># Use ggplot2 to visualize embarkment, passenger class, &amp; median fare</span>
ggplot<span class="p">(</span>embark_fare<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Embarked<span class="p">,</span> y <span class="o">=</span> Fare<span class="p">,</span> fill <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>Pclass<span class="p">)))</span> <span class="o">+</span>
  geom_boxplot<span class="p">()</span> <span class="o">+</span>
  geom_hline<span class="p">(</span>aes<span class="p">(</span>yintercept<span class="o">=</span><span class="m">80</span><span class="p">),</span> 
    colour<span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> linetype<span class="o">=</span><span class="s">&#39;dashed&#39;</span><span class="p">,</span> lwd<span class="o">=</span><span class="m">2</span><span class="p">)</span> <span class="o">+</span>
  scale_y_continuous<span class="p">()</span> <span class="o">+</span>
  theme_few<span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>## Warning: Removed 1 rows containing non-finite values (stat_boxplot).
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-9-1.png"><!-- --></p>
<p>Since their fare was $80 for 1st class, they most likely embarked from 'C'</p>
<div class="highlight"><pre><span></span>combine<span class="o">$</span>Embarked<span class="p">[</span><span class="kt">c</span><span class="p">(</span><span class="m">62</span><span class="p">,</span> <span class="m">830</span><span class="p">)]</span> <span class="o">&lt;-</span> <span class="s">&#39;C&#39;</span>
</pre></div>


<h4>5.3 Fare</h4>
<p>We will use media Fare group by Pcclass to fill missing Fare. Fare and PCClass have strong relationship.</p>
<div class="highlight"><pre><span></span>filter<span class="p">(</span>combine<span class="p">,</span> <span class="kp">is.na</span><span class="p">(</span>Fare<span class="p">))</span> 
</pre></div>


<div class="highlight"><pre><span></span>## 
## ----------------------------------------------------------------------------
##  PassengerId   Survived   Pclass          Name          Sex    Age    SibSp 
## ------------- ---------- -------- -------------------- ------ ------ -------
##     1044          NA        3      Storey, Mr. Thomas   male   60.5     0   
## ----------------------------------------------------------------------------
## 
## Table: Table continues below
## 
##  
## --------------------------------------------------
##  Parch   Ticket   Fare   Cabin   Embarked   Title 
## ------- -------- ------ ------- ---------- -------
##    0      3701     NA     NA        S        Mr   
## --------------------------------------------------
</pre></div>


<div class="highlight"><pre><span></span>med_fare_3 <span class="o">&lt;-</span> combine <span class="o">%&gt;%</span>
  filter<span class="p">(</span><span class="o">!</span><span class="kp">is.na</span><span class="p">(</span>Fare<span class="p">))</span> <span class="o">%&gt;%</span>
  group_by<span class="p">(</span>Pclass<span class="p">)</span> <span class="o">%&gt;%</span> 
  summarise<span class="p">(</span>med_fare <span class="o">=</span> median<span class="p">(</span>Fare<span class="p">))</span> <span class="o">%&gt;%</span>
  filter<span class="p">(</span>Pclass <span class="o">==</span> <span class="m">3</span><span class="p">)</span> <span class="o">%&gt;%</span>
  <span class="m">.</span><span class="o">$</span>med_fare

combine <span class="o">&lt;-</span> combine <span class="o">%&gt;%</span>
  mutate<span class="p">(</span>Fare <span class="o">=</span> case_when<span class="p">(</span>
    <span class="kp">is.na</span><span class="p">(</span>Fare<span class="p">)</span> <span class="o">~</span> med_fare_3<span class="p">,</span>
    <span class="kc">TRUE</span> <span class="o">~</span> Fare
  <span class="p">))</span>
</pre></div>


<h4>5.4 Age</h4>
<p>We will use media age from other passanger group by Tittle, Age have strong relationship with tittle.</p>
<div class="highlight"><pre><span></span>group_by<span class="p">(</span>combine<span class="p">,</span> Title<span class="p">)</span> <span class="o">%&gt;%</span>
        summarise<span class="p">(</span>median <span class="o">=</span> median<span class="p">(</span>Age<span class="p">,</span> na.rm <span class="o">=</span> <span class="kc">TRUE</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>## 
## -----------------
##  Title    median 
## -------- --------
##  Master     4    
## 
##   Miss      22   
## 
##    Mr       29   
## 
##   Mrs       35   
## 
##  Others     48   
## -----------------
</pre></div>


<div class="highlight"><pre><span></span>combine <span class="o">&lt;-</span> combine <span class="o">%&gt;%</span>
              group_by<span class="p">(</span>Title<span class="p">)</span> <span class="o">%&gt;%</span>
              mutate<span class="p">(</span>Age <span class="o">=</span> <span class="kp">ifelse</span><span class="p">(</span><span class="kp">is.na</span><span class="p">(</span>Age<span class="p">),</span> <span class="kp">round</span><span class="p">(</span>median<span class="p">(</span>Age<span class="p">,</span> na.rm <span class="o">=</span> <span class="kc">TRUE</span><span class="p">),</span> <span class="m">1</span><span class="p">),</span> Age<span class="p">))</span> 
</pre></div>


<h3>6. what are first three algorithms that you will to create prediction model for this dataset?</h3>
<ol>
<li>Decision Tree</li>
<li>Logistic Regression</li>
<li>XGBoost</li>
</ol>
<h4>6.1 Decision Tree</h4>
<p>A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of <em>feature importance</em> from a trained predictive model.</p>
<div class="highlight"><pre><span></span>Model_DT<span class="o">=</span>rpart<span class="p">(</span>Survived<span class="o">~</span><span class="m">.</span><span class="p">,</span>data<span class="o">=</span>train_val<span class="p">,</span>method<span class="o">=</span><span class="s">&quot;class&quot;</span><span class="p">)</span>
rpart.plot<span class="p">(</span>Model_DT<span class="p">,</span>extra <span class="o">=</span>  <span class="m">3</span><span class="p">,</span>fallen.leaves <span class="o">=</span> <span class="bp">T</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-16-1.png"><!-- --></p>
<div class="highlight"><pre><span></span>PRE_TDT<span class="o">=</span>predict<span class="p">(</span>Model_DT<span class="p">,</span>data<span class="o">=</span>train_val<span class="p">,</span>type<span class="o">=</span><span class="s">&quot;class&quot;</span><span class="p">)</span>
<span class="c1">#confusionMatrix(PRE_TDT,train_val$Survived)</span>
</pre></div>


<div class="highlight"><pre><span></span>confusionMatrix<span class="p">(</span>PRE_TDT<span class="p">,</span>train_val<span class="o">$</span>Survived<span class="p">)</span><span class="o">$</span>overall<span class="p">[</span><span class="m">1</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>##  Accuracy 
## 0.8417367
</pre></div>


<h4>6.2 Logistic Regression</h4>
<p>We can also use logistic regression to understand the functional relationship between the independent variables and the dependent variable, to try to understand what might cause the probability of the dependent variable to change</p>
<div class="highlight"><pre><span></span>log.mod <span class="o">&lt;-</span> glm<span class="p">(</span>Survived <span class="o">~</span> <span class="m">.</span><span class="p">,</span> family <span class="o">=</span> binomial<span class="p">(</span>link<span class="o">=</span>logit<span class="p">),</span> 
               data <span class="o">=</span> train_val<span class="p">)</span>
<span class="c1">###Check the summary</span>
<span class="kp">summary</span><span class="p">(</span>log.mod<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>## 
## Call:
## glm(formula = Survived ~ ., family = binomial(link = logit), 
##     data = train_val)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4137  -0.5832  -0.3970   0.5420   2.4913  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  18.084814 535.412123   0.034   0.9731    
## Pclass2      -0.807673   0.357734  -2.258   0.0240 *  
## Pclass3      -1.919969   0.356317  -5.388 7.11e-08 ***
## Sexmale     -14.012905 535.411648  -0.026   0.9791    
## Age          -0.026667   0.010885  -2.450   0.0143 *  
## SibSp        -0.567958   0.142798  -3.977 6.97e-05 ***
## Parch        -0.332783   0.146378  -2.273   0.0230 *  
## Fare          0.006090   0.003488   1.746   0.0808 .  
## EmbarkedQ     0.041227   0.443593   0.093   0.9260    
## EmbarkedS    -0.366009   0.271234  -1.349   0.1772    
## TitleMiss   -14.725697 535.411961  -0.028   0.9781    
## TitleMr      -3.482775   0.622842  -5.592 2.25e-08 ***
## TitleMrs    -13.884327 535.412100  -0.026   0.9793    
## TitleOthers  -3.799501   0.933596  -4.070 4.71e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 950.86  on 713  degrees of freedom
## Residual deviance: 593.40  on 700  degrees of freedom
## AIC: 621.4
## 
## Number of Fisher Scoring iterations: 12
</pre></div>


<div class="highlight"><pre><span></span>test.probs <span class="o">&lt;-</span> predict<span class="p">(</span>log.mod<span class="p">,</span> newdata<span class="o">=</span>test_val<span class="p">,</span>type <span class="o">=</span>  <span class="s">&quot;response&quot;</span><span class="p">)</span>
<span class="kp">table</span><span class="p">(</span>test_val<span class="o">$</span>Survived<span class="p">,</span>test.probs<span class="o">&gt;</span><span class="m">0.5</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>##    
##     FALSE TRUE
##   0    98   11
##   1    16   52
</pre></div>


<div class="highlight"><pre><span></span>print <span class="p">(</span><span class="s">&quot;Accuracy: &quot;</span><span class="p">);</span> <span class="p">(</span><span class="m">96+49</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="m">96+13+19+49</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>## [1] &quot;Accuracy: &quot;
## [1] 0.819209
</pre></div>


<h4>6.3 XGBoost</h4>
<p>XGBoost is used in a number of winning Kaggle solutions. XGBoost employs a number of tricks that make it faster and more accurate than traditional Algoritma. Benefit of using ensembles of decision tree methods like XGBoost is that they can automatically provide estimates of feature importance from a trained predictive model.</p>
<div class="highlight"><pre><span></span><span class="c1"># Params for xgboost</span>
param <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">(</span>booster <span class="o">=</span> <span class="s">&quot;gbtree&quot;</span><span class="p">,</span>
              <span class="c1">#eval_metric = &quot;auc&quot;, </span>
              objective <span class="o">=</span> <span class="s">&quot;binary:logistic&quot;</span><span class="p">,</span>
              eta <span class="o">=</span> <span class="m">.11</span><span class="p">,</span>
              gamma <span class="o">=</span> <span class="m">1</span><span class="p">,</span>
              max_depth <span class="o">=</span> <span class="m">6</span><span class="p">,</span>
              min_child_weight <span class="o">=</span> <span class="m">1</span><span class="p">,</span>
              subsample <span class="o">=</span> <span class="m">.7</span><span class="p">,</span>
              colsample_bytree <span class="o">=</span> <span class="m">.7</span><span class="p">)</span>

xgb_model <span class="o">&lt;-</span> xgb.train<span class="p">(</span>data <span class="o">=</span> dtrain<span class="p">,</span>
                       params <span class="o">=</span> param<span class="p">,</span>
                       watchlist <span class="o">=</span> <span class="kt">list</span><span class="p">(</span>train <span class="o">=</span> dtrain<span class="p">,</span> test <span class="o">=</span> dtest<span class="p">),</span>
                       nrounds <span class="o">=</span> <span class="m">72</span><span class="p">,</span>
                       verbose <span class="o">=</span> <span class="m">1</span><span class="p">,</span>
                       print_every_n <span class="o">=</span> <span class="m">9</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>## [1]  train-error:0.142857    test-error:0.169492 
## [10] train-error:0.121849    test-error:0.158192 
## [19] train-error:0.116246    test-error:0.175141 
## [28] train-error:0.105042    test-error:0.158192 
## [37] train-error:0.099440    test-error:0.169492 
## [46] train-error:0.100840    test-error:0.158192 
## [55] train-error:0.098039    test-error:0.163842 
## [64] train-error:0.096639    test-error:0.152542 
## [72] train-error:0.093838    test-error:0.163842
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Get the feature real names</span>
names <span class="o">&lt;-</span> <span class="kp">dimnames</span><span class="p">(</span>select<span class="p">(</span>train_val<span class="p">,</span> <span class="o">-</span><span class="kt">c</span><span class="p">(</span>Survived<span class="p">)))[[</span><span class="m">2</span><span class="p">]]</span>

<span class="c1"># Compute feature importance matrix</span>
importance_matrix <span class="o">&lt;-</span> xgb.importance<span class="p">(</span><span class="kp">names</span><span class="p">,</span> model <span class="o">=</span> xgb_model<span class="p">)</span>

<span class="c1"># Plotting</span>
xgb.plot.importance<span class="p">(</span>importance_matrix<span class="p">)</span>
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-24-1.png"><!-- --></p>
<h3>7. Do you notice any outliers or noise in the data?</h3>
<p>There is no outlier in this dataset, there is passanger that buy ticket for $500 but I think its still normal becouse he/she in claass 1. To examine outliyer we can use box plot to explore. </p>
<div class="highlight"><pre><span></span>ggplot<span class="p">(</span>train<span class="p">,</span> aes<span class="p">(</span>Pclass<span class="p">,</span> Fare<span class="p">,</span> colour <span class="o">=</span> Survived<span class="p">))</span> <span class="o">+</span>
  geom_boxplot<span class="p">()</span>
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-25-1.png"><!-- --></p>
<h3>8. How do you measure the quality of your prediction?</h3>
<p>binary classification model predicts the probability of a target variable to be Yes/No. To evaluate such a model, a metric called the confusion matrix is used, also called the classification or co-incidence matrix. With the help of a confusion matrix, we can calculate important performance measures</p>
<div class="highlight"><pre><span></span>confusionMatrix<span class="p">(</span>PRE_TDT<span class="p">,</span>train_val<span class="o">$</span>Survived<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 403  76
##          1  37 198
##                                           
##                Accuracy : 0.8417          
##                  95% CI : (0.8129, 0.8678)
##     No Information Rate : 0.6162          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6562          
##  Mcnemar&#39;s Test P-Value : 0.0003506       
##                                           
##             Sensitivity : 0.9159          
##             Specificity : 0.7226          
##          Pos Pred Value : 0.8413          
##          Neg Pred Value : 0.8426          
##              Prevalence : 0.6162          
##          Detection Rate : 0.5644          
##    Detection Prevalence : 0.6709          
##       Balanced Accuracy : 0.8193          
##                                           
##        &#39;Positive&#39; Class : 0               
## 
</pre></div>


<h3>9. Are there any insights you get when you explore the data?</h3>
<p>If you are man, dont be 3rd class persone becouse you probably will die in competition</p>
<div class="highlight"><pre><span></span>ggplot<span class="p">(</span>combine<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">891</span><span class="p">,],</span> aes<span class="p">(</span>x <span class="o">=</span> Sex<span class="p">,</span> fill <span class="o">=</span> Survived<span class="p">))</span> <span class="o">+</span>
geom_bar<span class="p">()</span> <span class="o">+</span>
facet_wrap<span class="p">(</span><span class="o">~</span>Pclass<span class="p">)</span> <span class="o">+</span> 
ggtitle<span class="p">(</span><span class="s">&quot;sex, pclass, and survival&quot;</span><span class="p">)</span> <span class="o">+</span>
xlab<span class="p">(</span><span class="s">&quot;Sex&quot;</span><span class="p">)</span> <span class="o">+</span>
ylab<span class="p">(</span><span class="s">&quot;Total Count&quot;</span><span class="p">)</span> <span class="o">+</span>
labs<span class="p">(</span>fill <span class="o">=</span> <span class="s">&quot;Survived&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="images/titanic-analysis_files/figure-html/unnamed-chunk-27-1.png"><!-- --></p>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
      <li class="list-inline-item"><a href="/authors.html">Authors</a></li>
    <li class="list-inline-item"><a href="/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>